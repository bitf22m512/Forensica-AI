{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forensica-AI: Deepfake Detection Training Notebook\n",
    "\n",
    "This notebook contains all the code needed to train a deepfake detection model on Google Colab.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Paste your Kaggle dataset link in the cell below\n",
    "3. Run all cells to start training\n",
    "\n",
    "The model uses a CNN-RNN architecture to classify videos as real or fake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:35:51.061165Z",
     "iopub.status.busy": "2026-01-24T14:35:51.060622Z",
     "iopub.status.idle": "2026-01-24T14:35:54.360815Z",
     "shell.execute_reply": "2026-01-24T14:35:54.359916Z",
     "shell.execute_reply.started": "2026-01-24T14:35:51.061134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -q kaggle opencv-python torch torchvision tqdm pyyaml scikit-learn pillow pandas numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install facenet-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mtcnn = MTCNN(\n",
    "    image_size=128,\n",
    "    margin=20,\n",
    "    keep_all=False,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Dataset from Kaggle\n",
    "\n",
    "Paste your Kaggle dataset link here. The dataset should contain videos in a structure like:\n",
    "- `Celeb-real/` folder with real videos\n",
    "- `Celeb-synthesis/` folder with fake videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:36:01.857513Z",
     "iopub.status.busy": "2026-01-24T14:36:01.856738Z",
     "iopub.status.idle": "2026-01-24T14:36:01.866711Z",
     "shell.execute_reply": "2026-01-24T14:36:01.866055Z",
     "shell.execute_reply.started": "2026-01-24T14:36:01.857433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Directories created!\n",
      "‚ö†Ô∏è  Kaggle download failed: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n",
      "   Please upload your dataset manually to 'data/raw_videos' folder\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# KAGGLE DATASET DOWNLOAD\n",
    "# ============================================\n",
    "# Option 1: Using Kaggle API (recommended)\n",
    "# First, upload your kaggle.json file or set credentials:\n",
    "# from google.colab import files\n",
    "# files.upload()  # Upload kaggle.json\n",
    "\n",
    "# Option 2: Direct download link (paste your dataset link)\n",
    "# Option 3: Manual upload via Colab file browser\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure your dataset here\n",
    "KAGGLE_DATASET = \"/kaggle/input/celeb-df-v2\"  # Format: \"username/dataset-name\" (e.g., \"tunguz/deepfake-detection\")\n",
    "DATASET_URL = \"\"  # Or paste direct download URL here\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"data/raw_videos\", exist_ok=True)\n",
    "os.makedirs(\"data/frames\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Directories created!\")\n",
    "\n",
    "# Download dataset if Kaggle dataset name is provided\n",
    "if KAGGLE_DATASET:\n",
    "    try:\n",
    "        import kaggle\n",
    "        print(f\"üì• Downloading dataset: {KAGGLE_DATASET}\")\n",
    "        kaggle.api.dataset_download_files(KAGGLE_DATASET, path=\"data/\", unzip=True)\n",
    "        print(\"‚úÖ Dataset downloaded!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Kaggle download failed: {e}\")\n",
    "        print(\"   Please upload your dataset manually to 'data/raw_videos' folder\")\n",
    "elif DATASET_URL:\n",
    "    print(f\"üì• Downloading from URL: {DATASET_URL}\")\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(DATASET_URL, \"dataset.zip\")\n",
    "    with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data/\")\n",
    "    print(\"‚úÖ Dataset downloaded!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No dataset configured.\")\n",
    "    print(\"   Please either:\")\n",
    "    print(\"   1. Set KAGGLE_DATASET variable above (format: 'username/dataset-name')\")\n",
    "    print(\"   2. Set DATASET_URL variable above with direct download link\")\n",
    "    print(\"   3. Manually upload your dataset to the 'data/raw_videos' folder via Colab file browser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Definitions\n",
    "\n",
    "CNN Feature Extractor and RNN Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:36:08.275769Z",
     "iopub.status.busy": "2026-01-24T14:36:08.275025Z",
     "iopub.status.idle": "2026-01-24T14:36:08.311273Z",
     "shell.execute_reply": "2026-01-24T14:36:08.310429Z",
     "shell.execute_reply.started": "2026-01-24T14:36:08.275734Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Celeb-DF v2 dataset found\n",
      "Real videos (Celeb): 590\n",
      "Fake videos (Celeb): 5639\n",
      "Real videos (YouTube): 300\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATASET SETUP (Celeb-DF v2 - Attached Dataset)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Root directory where the dataset is mounted\n",
    "# (Kaggle/Colab-style input mount)\n",
    "DATASET_ROOT = Path(\"/kaggle/input/celeb-df-v2\")\n",
    "\n",
    "# Dataset subfolders\n",
    "CELEB_REAL_DIR = DATASET_ROOT / \"Celeb-real\"\n",
    "CELEB_FAKE_DIR = DATASET_ROOT / \"Celeb-synthesis\"\n",
    "YOUTUBE_REAL_DIR = DATASET_ROOT / \"YouTube-real\"\n",
    "\n",
    "TEST_LIST_FILE = DATASET_ROOT / \"List_of_testing_videos.txt\"\n",
    "\n",
    "# Sanity checks\n",
    "assert CELEB_REAL_DIR.exists(), \"Celeb-real folder not found\"\n",
    "assert CELEB_FAKE_DIR.exists(), \"Celeb-synthesis folder not found\"\n",
    "assert YOUTUBE_REAL_DIR.exists(), \"YouTube-real folder not found\"\n",
    "assert TEST_LIST_FILE.exists(), \"List_of_testing_videos.txt not found\"\n",
    "\n",
    "print(\"‚úÖ Celeb-DF v2 dataset found\")\n",
    "print(f\"Real videos (Celeb): {len(list(CELEB_REAL_DIR.glob('*.mp4')))}\")\n",
    "print(f\"Fake videos (Celeb): {len(list(CELEB_FAKE_DIR.glob('*.mp4')))}\")\n",
    "print(f\"Real videos (YouTube): {len(list(YOUTUBE_REAL_DIR.glob('*.mp4')))}\")\n",
    "\n",
    "# Unified raw video directory (used by later cells)\n",
    "RAW_VIDEO_DIR = DATASET_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:36:14.580054Z",
     "iopub.status.busy": "2026-01-24T14:36:14.579794Z",
     "iopub.status.idle": "2026-01-24T14:36:14.588981Z",
     "shell.execute_reply": "2026-01-24T14:36:14.588085Z",
     "shell.execute_reply.started": "2026-01-24T14:36:14.580033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CNN Feature Extractor defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CNN FEATURE EXTRACTOR (MobileNetV2 ‚Äì FIXED)\n",
    "# ============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MobileNetFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2-based feature extractor\n",
    "    Input:  [B*T, 3, H, W]\n",
    "    Output: [B*T, 256]\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "        # Backbone\n",
    "        self.backbone = mobilenet.features\n",
    "\n",
    "        # Freeze early layers\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Projection head\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "\n",
    "        self.output_dim = 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_cnn(use_custom=False):\n",
    "    \"\"\"\n",
    "    Build CNN feature extractor\n",
    "    \"\"\"\n",
    "    model = MobileNetFeatureExtractor(freeze_backbone=True)\n",
    "    return model, model.output_dim\n",
    "\n",
    "\n",
    "print(\"‚úÖ MobileNetV2 feature extractor ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:36:25.746087Z",
     "iopub.status.busy": "2026-01-24T14:36:25.745372Z",
     "iopub.status.idle": "2026-01-24T14:36:25.753746Z",
     "shell.execute_reply": "2026-01-24T14:36:25.752872Z",
     "shell.execute_reply.started": "2026-01-24T14:36:25.746058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RNN Classifier defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RNN CLASSIFIER\n",
    "# ============================================\n",
    "class RNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based video classifier.\n",
    "    Input:  [B, T, feature_dim]\n",
    "    Output: [B, 2] (real/fake logits)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim=256,\n",
    "                 hidden_size=128,\n",
    "                 num_layers=1,\n",
    "                 bidirectional=False,\n",
    "                 dropout=0.3):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=(dropout if num_layers > 1 else 0.0)\n",
    "        )\n",
    "\n",
    "        rnn_output_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(rnn_output_dim, 2)  # 2 classes: real/fake\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [B, T, feature_dim]\n",
    "        Returns: logits [B, 2]\n",
    "        \"\"\"\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            last_hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            last_hidden = h_n[-1]\n",
    "\n",
    "        logits = self.fc(last_hidden)\n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ RNN Classifier defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dataset and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:36:30.785033Z",
     "iopub.status.busy": "2026-01-24T14:36:30.784292Z",
     "iopub.status.idle": "2026-01-24T14:36:30.801263Z",
     "shell.execute_reply": "2026-01-24T14:36:30.800376Z",
     "shell.execute_reply.started": "2026-01-24T14:36:30.785005Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# VIDEO DATASET CLASS\n",
    "# ============================================\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NUM_FRAMES = 20\n",
    "FRAME_SIZE = 128\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"real\": 0,\n",
    "    \"fake\": 1,\n",
    "}\n",
    "\n",
    "frame_transform = transforms.Compose([\n",
    "    transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "class VideoSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset returning a fixed-length sequence of frames for each video.\n",
    "    \"\"\"\n",
    "    def __init__(self, frames_root: str, labels_csv: str, num_frames: int = NUM_FRAMES,\n",
    "                 transform=frame_transform, shuffle_frames: bool = False, cache_in_memory: bool = False):\n",
    "        self.frames_root = frames_root\n",
    "        self.labels_df = pd.read_csv(labels_csv)\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.shuffle_frames = shuffle_frames\n",
    "        self.cache_in_memory = cache_in_memory\n",
    "\n",
    "        self.samples: List[Tuple[str, int]] = []\n",
    "        for _, row in self.labels_df.iterrows():\n",
    "            video_filename = row[\"video\"]\n",
    "            video_folder = os.path.splitext(video_filename)[0]\n",
    "            folder_path = os.path.join(self.frames_root, video_folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            available = len([f for f in os.listdir(folder_path) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "            if available < self.num_frames:\n",
    "                continue\n",
    "            label_str = str(row[\"label\"]).lower()\n",
    "            if label_str not in LABEL_MAP:\n",
    "                continue\n",
    "            label_int = LABEL_MAP[label_str]\n",
    "            self.samples.append((video_folder, label_int))\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(\"No valid samples found. Check frames_root and labels_csv paths.\")\n",
    "\n",
    "        self._cache = {} if self.cache_in_memory else None\n",
    "        if self.cache_in_memory:\n",
    "            print(\"Caching frames in memory (may use lots of RAM)...\")\n",
    "            for vid, _ in self.samples:\n",
    "                folder = os.path.join(self.frames_root, vid)\n",
    "                frames = self._read_frames_from_folder(folder)\n",
    "                self._cache[vid] = frames\n",
    "\n",
    "    def _read_frames_from_folder(self, folder: str) -> List[Image.Image]:\n",
    "        \"\"\"Return list of PIL images sorted by frame index.\"\"\"\n",
    "        files = sorted([f for f in os.listdir(folder) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "        files = files[:self.num_frames]\n",
    "        images = []\n",
    "        for fname in files:\n",
    "            path = os.path.join(folder, fname)\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid_folder, label = self.samples[idx]\n",
    "        folder = os.path.join(self.frames_root, vid_folder)\n",
    "\n",
    "        if self.cache_in_memory and vid_folder in self._cache:\n",
    "            pil_frames = self._cache[vid_folder]\n",
    "        else:\n",
    "            pil_frames = self._read_frames_from_folder(folder)\n",
    "\n",
    "        if self.shuffle_frames:\n",
    "            pil_frames = pil_frames.copy()\n",
    "            random.shuffle(pil_frames)\n",
    "\n",
    "        frame_tensors = []\n",
    "        for f in pil_frames:\n",
    "            t = self.transform(f)\n",
    "            frame_tensors.append(t)\n",
    "\n",
    "        seq_tensor = torch.stack(frame_tensors, dim=0)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return seq_tensor, label_tensor\n",
    "\n",
    "def video_collate_fn(batch):\n",
    "    \"\"\"Collate function for video sequences.\"\"\"\n",
    "    seqs = [item[0] for item in batch]\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "    batch_seqs = torch.stack(seqs, dim=0)\n",
    "    return batch_seqs, labels\n",
    "\n",
    "def build_loaders(frames_root: str, labels_csv: str, batch_size: int = 4, train_split: float = 0.8,\n",
    "                  num_workers: int = 2, balanced_sampling: bool = True, **dataset_kwargs):\n",
    "    dataset = VideoSequenceDataset(frames_root=frames_root, labels_csv=labels_csv, **dataset_kwargs)\n",
    "    n = len(dataset)\n",
    "    n_train = int(n * train_split)\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:]\n",
    "\n",
    "    train_set = Subset(dataset, train_idx)\n",
    "    val_set = Subset(dataset, val_idx)\n",
    "\n",
    "    if balanced_sampling:\n",
    "        # Build a WeightedRandomSampler to balance real/fake classes in each batch\n",
    "        train_labels = [dataset.samples[i][1] for i in train_idx]\n",
    "        class_counts = np.bincount(train_labels, minlength=len(LABEL_MAP))\n",
    "        # Avoid division by zero\n",
    "        class_counts = np.where(class_counts == 0, 1, class_counts)\n",
    "        class_weights = 1.0 / class_counts\n",
    "        sample_weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=video_collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    else:\n",
    "        train_loader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=video_collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=video_collate_fn,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "print(\"‚úÖ Dataset class defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation\n",
    "\n",
    "Prepare labels CSV and extract frames from videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:36:37.330768Z",
     "iopub.status.busy": "2026-01-24T14:36:37.329775Z",
     "iopub.status.idle": "2026-01-24T14:36:37.368958Z",
     "shell.execute_reply": "2026-01-24T14:36:37.368215Z",
     "shell.execute_reply.started": "2026-01-24T14:36:37.330734Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ labels.csv created successfully\n",
      "üìä Total videos: 6529\n",
      "   Real: 890\n",
      "   Fake: 5639\n",
      "üö´ Test videos excluded: 518\n",
      "üìÑ Labels CSV saved at: data/labels.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PREPARE LABELS CSV (Celeb-DF v2 ‚Äì Correct)\n",
    "# ============================================\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_labels_csv(\n",
    "    dataset_root=\"/kaggle/input/celeb-df-v2\",\n",
    "    labels_csv=\"data/labels.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare labels.csv for Celeb-DF v2\n",
    "    \n",
    "    REAL  = Celeb-real + YouTube-real\n",
    "    FAKE  = Celeb-synthesis\n",
    "    TEST  = excluded using List_of_testing_videos.txt\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_root = Path(dataset_root)\n",
    "    labels_path = Path(labels_csv)\n",
    "    labels_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    celeb_real_dir = dataset_root / \"Celeb-real\"\n",
    "    youtube_real_dir = dataset_root / \"YouTube-real\"\n",
    "    celeb_fake_dir = dataset_root / \"Celeb-synthesis\"\n",
    "    test_list_file = dataset_root / \"List_of_testing_videos.txt\"\n",
    "\n",
    "    # Sanity checks\n",
    "    assert celeb_real_dir.exists(), \"Celeb-real folder missing\"\n",
    "    assert youtube_real_dir.exists(), \"YouTube-real folder missing\"\n",
    "    assert celeb_fake_dir.exists(), \"Celeb-synthesis folder missing\"\n",
    "    assert test_list_file.exists(), \"List_of_testing_videos.txt missing\"\n",
    "\n",
    "    # Load official test videos\n",
    "    with test_list_file.open(\"r\") as f:\n",
    "        test_videos = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    # REAL videos (Celeb-real)\n",
    "    for video_path in celeb_real_dir.glob(\"*.mp4\"):\n",
    "        if video_path.name not in test_videos:\n",
    "            samples.append((video_path.name, \"real\"))\n",
    "\n",
    "    # REAL videos (YouTube-real)\n",
    "    for video_path in youtube_real_dir.glob(\"*.mp4\"):\n",
    "        if video_path.name not in test_videos:\n",
    "            samples.append((video_path.name, \"real\"))\n",
    "\n",
    "    # FAKE videos (Celeb-synthesis)\n",
    "    for video_path in celeb_fake_dir.glob(\"*.mp4\"):\n",
    "        if video_path.name not in test_videos:\n",
    "            samples.append((video_path.name, \"fake\"))\n",
    "\n",
    "    if not samples:\n",
    "        raise RuntimeError(\"No training videos found after filtering test set.\")\n",
    "\n",
    "    # Write CSV\n",
    "    with labels_path.open(\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"video\", \"label\"])\n",
    "        writer.writerows(samples)\n",
    "\n",
    "    print(\"‚úÖ labels.csv created successfully\")\n",
    "    print(f\"üìä Total videos: {len(samples)}\")\n",
    "    print(f\"   Real: {sum(1 for _, l in samples if l == 'real')}\")\n",
    "    print(f\"   Fake: {sum(1 for _, l in samples if l == 'fake')}\")\n",
    "    print(f\"üö´ Test videos excluded: {len(test_videos)}\")\n",
    "\n",
    "    return labels_path\n",
    "\n",
    "\n",
    "# Run preparation\n",
    "labels_csv_path = prepare_labels_csv()\n",
    "print(f\"üìÑ Labels CSV saved at: {labels_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:36:44.080635Z",
     "iopub.status.busy": "2026-01-24T14:36:44.079892Z",
     "iopub.status.idle": "2026-01-24T14:56:07.809848Z",
     "shell.execute_reply": "2026-01-24T14:56:07.808954Z",
     "shell.execute_reply.started": "2026-01-24T14:36:44.080604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Extracting frames from 6529 videos\n",
      "   Frames/video: 20, Size: 128x128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6529/6529 [19:23<00:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Frame extraction finished\n",
      "   Successfully processed: 6528\n",
      "   Skipped: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXTRACT FRAMES FROM VIDEOS (Celeb-DF v2) ‚Äì FIXED\n",
    "# ============================================\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Load OpenCV face detector\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "def extract_frames_from_video(video_path, output_dir, num_frames=20, frame_size=128):\n",
    "    if not video_path.exists():\n",
    "        return False\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        return False\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames < num_frames + 10:\n",
    "        cap.release()\n",
    "        return False\n",
    "\n",
    "    # Skip first 10% frames (often junk)\n",
    "    start_frame = int(0.1 * total_frames)\n",
    "    usable_frames = total_frames - start_frame\n",
    "    interval = max(1, usable_frames // num_frames)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    extracted = 0\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        frame_idx = start_frame + i * interval\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Convert BGR ‚Üí RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect face\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            x, y, w, h = faces[0]\n",
    "            frame = frame[y:y+h, x:x+w]\n",
    "\n",
    "        frame = cv2.resize(frame, (frame_size, frame_size))\n",
    "        out_path = output_dir / f\"frame_{i}.jpg\"\n",
    "\n",
    "        if cv2.imwrite(str(out_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)):\n",
    "            extracted += 1\n",
    "\n",
    "    cap.release()\n",
    "    return extracted == num_frames\n",
    "\n",
    "\n",
    "def extract_all_frames(\n",
    "    labels_csv=\"data/labels.csv\",\n",
    "    dataset_root=\"/kaggle/input/celeb-df-v2\",\n",
    "    frames_dir=\"data/frames\",\n",
    "    num_frames=20,\n",
    "    frame_size=128\n",
    "):\n",
    "    labels = pd.read_csv(labels_csv)\n",
    "    dataset_root = Path(dataset_root)\n",
    "    frames_dir = Path(frames_dir)\n",
    "    frames_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    success_count, skip_count = 0, 0\n",
    "\n",
    "    for _, row in tqdm(labels.iterrows(), total=len(labels), desc=\"Extracting\"):\n",
    "        video_name = row[\"video\"]\n",
    "\n",
    "        candidate_paths = [\n",
    "            dataset_root / \"Celeb-real\" / video_name,\n",
    "            dataset_root / \"YouTube-real\" / video_name,\n",
    "            dataset_root / \"Celeb-synthesis\" / video_name,\n",
    "        ]\n",
    "\n",
    "        video_path = next((p for p in candidate_paths if p.exists()), None)\n",
    "        if video_path is None:\n",
    "            skip_count += 1\n",
    "            continue\n",
    "\n",
    "        output_dir = frames_dir / video_path.stem\n",
    "        ok = extract_frames_from_video(\n",
    "            video_path,\n",
    "            output_dir,\n",
    "            num_frames=num_frames,\n",
    "            frame_size=frame_size\n",
    "        )\n",
    "\n",
    "        if ok:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            skip_count += 1\n",
    "\n",
    "    print(\"\\n‚úÖ Frame extraction completed\")\n",
    "    print(f\"   Success: {success_count}\")\n",
    "    print(f\"   Skipped: {skip_count}\")\n",
    "\n",
    "    return success_count, skip_count\n",
    "\n",
    "\n",
    "# RUN\n",
    "success, skipped = extract_all_frames()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:56:56.792891Z",
     "iopub.status.busy": "2026-01-24T14:56:56.792045Z",
     "iopub.status.idle": "2026-01-24T14:56:56.798156Z",
     "shell.execute_reply": "2026-01-24T14:56:56.797390Z",
     "shell.execute_reply.started": "2026-01-24T14:56:56.792858Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Training Configuration:\n",
      "   batch_size: 4\n",
      "   num_frames: 20\n",
      "   frame_size: 128\n",
      "   learning_rate: 0.0003\n",
      "   epochs: 15\n",
      "   lstm_hidden_size: 128\n",
      "   num_layers: 1\n",
      "   train_split: 0.8\n",
      "   frames_dir: data/frames\n",
      "   labels_csv: data/labels.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "# You can modify these hyperparameters\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": 4,\n",
    "    \"num_frames\": 20,\n",
    "    \"frame_size\": 128,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"epochs\": 15,\n",
    "    \"lstm_hidden_size\": 128,\n",
    "    \"num_layers\": 1,\n",
    "    \"train_split\": 0.8,\n",
    "    \"frames_dir\": \"data/frames\",\n",
    "    \"labels_csv\": \"data/labels.csv\",\n",
    "}\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:57:02.496643Z",
     "iopub.status.busy": "2026-01-24T14:57:02.496266Z",
     "iopub.status.idle": "2026-01-24T14:57:02.506315Z",
     "shell.execute_reply": "2026-01-24T14:57:02.505703Z",
     "shell.execute_reply.started": "2026-01-24T14:57:02.496617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(cnn, rnn, loader, criterion, optimizer, device):\n",
    "    cnn.train()\n",
    "    rnn.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for seqs, labels in tqdm(loader, desc=\"Training\", ncols=100):\n",
    "        seqs = seqs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        B, T, C, H, W = seqs.shape\n",
    "        seqs_reshaped = seqs.view(B * T, C, H, W)\n",
    "        features = cnn(seqs_reshaped)\n",
    "        feature_dim = features.shape[-1]\n",
    "        features = features.view(B, T, feature_dim)\n",
    "\n",
    "        logits = rnn(features)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += B\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def validate(cnn, rnn, loader, criterion, device):\n",
    "    cnn.eval()\n",
    "    rnn.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seqs, labels in tqdm(loader, desc=\"Validating\", ncols=100):\n",
    "            seqs = seqs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            B, T, C, H, W = seqs.shape\n",
    "            seqs_reshaped = seqs.view(B * T, C, H, W)\n",
    "            features = cnn(seqs_reshaped)\n",
    "            feature_dim = features.shape[-1]\n",
    "            features = features.view(B, T, feature_dim)\n",
    "\n",
    "            logits = rnn(features)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * B\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += B\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "print(\"‚úÖ Training functions defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Start Training\n",
    "\n",
    "Run this cell to begin training your model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T14:57:09.063649Z",
     "iopub.status.busy": "2026-01-24T14:57:09.062870Z",
     "iopub.status.idle": "2026-01-24T18:14:08.620810Z",
     "shell.execute_reply": "2026-01-24T18:14:08.619919Z",
     "shell.execute_reply.started": "2026-01-24T14:57:09.063617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cuda\n",
      "\n",
      "üì¶ Building data loaders...\n",
      "   Train samples: 5222\n",
      "   Val samples: 1306\n",
      "\n",
      "üèóÔ∏è  Building models...\n",
      "   CNN feature dim: 256\n",
      "   RNN hidden size: 128\n",
      "\n",
      "üöÄ Starting training for 15 epochs...\n",
      "============================================================\n",
      "\n",
      "--- Epoch 1/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [12:38<00:00,  1.72it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4079 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3715 | Val   Acc: 0.8783\n",
      "‚úÖ Best model saved (acc=0.8783) at epoch 1\n",
      "\n",
      "--- Epoch 2/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [12:14<00:00,  1.78it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4046 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3817 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 3/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [12:13<00:00,  1.78it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4057 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3710 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 4/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [12:15<00:00,  1.78it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4054 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3705 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 5/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [12:13<00:00,  1.78it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4054 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3705 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 6/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [12:13<00:00,  1.78it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4056 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3720 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 7/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [12:16<00:00,  1.77it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4056 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3712 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 8/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [12:16<00:00,  1.77it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4054 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3734 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 9/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [11:50<00:00,  1.84it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:05<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4057 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3704 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 10/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [11:41<00:00,  1.86it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:05<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4050 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3723 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 11/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [11:42<00:00,  1.86it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:05<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4054 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3774 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 12/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [11:41<00:00,  1.86it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:05<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4053 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3740 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 13/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [11:44<00:00,  1.85it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:05<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4058 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3725 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 14/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [11:44<00:00,  1.85it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:05<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4055 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3717 | Val   Acc: 0.8783\n",
      "\n",
      "--- Epoch 15/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1306/1306 [11:44<00:00,  1.85it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:06<00:00,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4054 | Train Acc: 0.8602\n",
      "Val   Loss: 0.3709 | Val   Acc: 0.8783\n",
      "\n",
      "============================================================\n",
      "üéâ Training complete!\n",
      "Best validation accuracy: 0.8783\n",
      "Model saved to: models/best_model.pth\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "# Build data loaders\n",
    "print(\"\\nüì¶ Building data loaders...\")\n",
    "train_loader, val_loader = build_loaders(\n",
    "    frames_root=CONFIG[\"frames_dir\"],\n",
    "    labels_csv=CONFIG[\"labels_csv\"],\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    train_split=CONFIG[\"train_split\"],\n",
    "    num_workers=2,\n",
    "    num_frames=CONFIG[\"num_frames\"],\n",
    "    cache_in_memory=False,\n",
    "    balanced_sampling=True,\n",
    ")\n",
    "\n",
    "print(f\"   Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"   Val samples: {len(val_loader.dataset)}\")\n",
    "\n",
    "# Build models\n",
    "print(\"\\nüèóÔ∏è  Building models...\")\n",
    "cnn, feature_dim = build_cnn()\n",
    "\n",
    "cnn.to(device)\n",
    "\n",
    "rnn = RNNClassifier(\n",
    "    feature_dim=feature_dim,\n",
    "    hidden_size=CONFIG[\"lstm_hidden_size\"],\n",
    "    num_layers=CONFIG[\"num_layers\"],\n",
    "    bidirectional=False\n",
    ")\n",
    "rnn.to(device)\n",
    "\n",
    "print(f\"   CNN feature dim: {feature_dim}\")\n",
    "print(f\"   RNN hidden size: {CONFIG['lstm_hidden_size']}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "# Stronger weighting for the minority REAL class to avoid always predicting FAKE\n",
    "class_weights = torch.tensor([3.0, 1.0]).float().to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(cnn.parameters()) + list(rnn.parameters()),\n",
    "    lr=CONFIG[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nüöÄ Starting training for {CONFIG['epochs']} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "SAVE_DIR = \"models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{CONFIG['epochs']} ---\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        cnn, rnn, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    val_loss, val_acc = validate(\n",
    "        cnn, rnn, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_path = os.path.join(SAVE_DIR, \"best_model.pth\")\n",
    "        best_model_data = {\n",
    "            \"cnn_state\": cnn.state_dict(),\n",
    "            \"rnn_state\": rnn.state_dict(),\n",
    "            \"feature_dim\": feature_dim,\n",
    "            \"rnn_params\": {\n",
    "                \"feature_dim\": feature_dim,\n",
    "                \"hidden_size\": CONFIG[\"lstm_hidden_size\"],\n",
    "                \"num_layers\": CONFIG[\"num_layers\"],\n",
    "                \"bidirectional\": False\n",
    "            },\n",
    "            \"config\": CONFIG,\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc\n",
    "        }\n",
    "        torch.save(best_model_data, best_model_path)\n",
    "        print(f\"‚úÖ Best model saved (acc={val_acc:.4f}) at epoch {epoch + 1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Training complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Model saved to: {os.path.join(SAVE_DIR, 'best_model.pth')}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T18:16:18.753016Z",
     "iopub.status.busy": "2026-01-24T18:16:18.752286Z",
     "iopub.status.idle": "2026-01-24T18:17:27.067240Z",
     "shell.execute_reply": "2026-01-24T18:17:27.066384Z",
     "shell.execute_reply.started": "2026-01-24T18:16:18.752983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [01:07<00:00,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.00      0.00      0.00       159\n",
      "        fake       0.88      1.00      0.94      1147\n",
      "\n",
      "    accuracy                           0.88      1306\n",
      "   macro avg       0.44      0.50      0.47      1306\n",
      "weighted avg       0.77      0.88      0.82      1306\n",
      "\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "[[   0  159]\n",
      " [   0 1147]]\n",
      "\n",
      "‚úÖ Final Validation Accuracy: 0.8783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EVALUATION\n",
    "# ============================================\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "MODEL_PATH = \"models/best_model.pth\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"üìä Evaluating model...\")\n",
    "    \n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    feature_dim = checkpoint[\"feature_dim\"]\n",
    "    \n",
    "    cnn, _ = build_cnn(use_custom=True)\n",
    "    rnn = RNNClassifier(feature_dim=feature_dim)\n",
    "    \n",
    "    cnn.load_state_dict(checkpoint[\"cnn_state\"])\n",
    "    rnn.load_state_dict(checkpoint[\"rnn_state\"])\n",
    "    \n",
    "    cnn.to(device)\n",
    "    rnn.to(device)\n",
    "    \n",
    "    cnn.eval()\n",
    "    rnn.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seqs, labels in tqdm(val_loader, desc=\"Evaluating\", ncols=100):\n",
    "            seqs = seqs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            B, T, C, H, W = seqs.shape\n",
    "            seqs_reshaped = seqs.view(B*T, C, H, W)\n",
    "            \n",
    "            features = cnn(seqs_reshaped)\n",
    "            features = features.view(B, T, -1)\n",
    "            \n",
    "            logits = rnn(features)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    print(\"\\nüìà Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"real\", \"fake\"]))\n",
    "    \n",
    "    print(\"\\nüìä Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "    \n",
    "    acc = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "    print(f\"\\n‚úÖ Final Validation Accuracy: {acc:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå Model not found. Please train the model first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Inference on New Videos\n",
    "\n",
    "Use this cell to predict on a single video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T18:45:20.973993Z",
     "iopub.status.busy": "2026-01-24T18:45:20.973327Z",
     "iopub.status.idle": "2026-01-24T18:45:21.583687Z",
     "shell.execute_reply": "2026-01-24T18:45:21.582962Z",
     "shell.execute_reply.started": "2026-01-24T18:45:20.973962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üé¨ Video Prediction\n",
      "========================================\n",
      "Video: /kaggle/input/celeb-df-v2/Celeb-real/id0_0008.mp4\n",
      "Prediction: FAKE\n",
      "Confidence: 0.8624\n",
      "========================================\n",
      "‚úÖ Inference function ready!\n",
      "   Use: predict_video('/kaggle/input/celeb-df-v2/Celeb-synthesis/id0_id16_0002.mp4')\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# INFERENCE ON SINGLE VIDEO\n",
    "# ============================================\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def extract_frames_inference(video_path, num_frames=20):\n",
    "    \"\"\"Extract frames from a single video for inference.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if total < num_frames:\n",
    "        raise ValueError(\"Video too short for inference!\")\n",
    "    \n",
    "    interval = total // num_frames\n",
    "    frames = []\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def predict_video(video_path, model_path=\"models/best_model.pth\"):\n",
    "    \"\"\"Predict if a video is real or fake.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"‚ùå Model not found. Please train the model first.\")\n",
    "        return None\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    feature_dim = checkpoint[\"feature_dim\"]\n",
    "    \n",
    "    cnn, _ = build_cnn()\n",
    "\n",
    "    rnn = RNNClassifier(feature_dim=feature_dim)\n",
    "    \n",
    "    cnn.load_state_dict(checkpoint[\"cnn_state\"])\n",
    "    rnn.load_state_dict(checkpoint[\"rnn_state\"])\n",
    "    \n",
    "    cnn.to(device)\n",
    "    rnn.to(device)\n",
    "    \n",
    "    cnn.eval()\n",
    "    rnn.eval()\n",
    "    \n",
    "    # Extract frames\n",
    "    frames = extract_frames_inference(video_path)\n",
    "    \n",
    "    # Preprocess\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    tensors = []\n",
    "    for f in frames:\n",
    "        tensors.append(transform(f))\n",
    "    \n",
    "    seq_tensor = torch.stack(tensors, dim=0)  # [T, C, H, W]\n",
    "    seq_tensor = seq_tensor.unsqueeze(0)      # [1, T, C, H, W]\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        B, T, C, H, W = seq_tensor.shape\n",
    "        seq_tensor = seq_tensor.to(device)\n",
    "        \n",
    "        reshaped = seq_tensor.view(B*T, C, H, W)\n",
    "        features = cnn(reshaped)\n",
    "        features = features.view(B, T, -1)\n",
    "        \n",
    "        logits = rnn(features)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "    \n",
    "    pred_class = torch.argmax(probs, dim=1).item()\n",
    "    confidence = probs[0][pred_class].item()\n",
    "    \n",
    "    label_map = {0: \"REAL\", 1: \"FAKE\"}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"üé¨ Video Prediction\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Video: {video_path}\")\n",
    "    print(f\"Prediction: {label_map[pred_class]}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    return label_map[pred_class], confidence\n",
    "\n",
    "# Example usage (uncomment and provide video path):\n",
    " #video_path = \"/kaggle/input/celeb-df-v2/Celeb-synthesis/id0_id16_0002.mp4\"\n",
    "predict_video('/kaggle/input/celeb-df-v2/Celeb-real/id0_0008.mp4')\n",
    "\n",
    "print(\"‚úÖ Inference function ready!\")\n",
    "print(\"   Use: predict_video('/kaggle/input/celeb-df-v2/Celeb-synthesis/id0_id16_0002.mp4')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Trained Model\n",
    "\n",
    "Download your trained model from Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T18:23:53.569721Z",
     "iopub.status.busy": "2026-01-24T18:23:53.569088Z",
     "iopub.status.idle": "2026-01-24T18:23:53.577934Z",
     "shell.execute_reply": "2026-01-24T18:23:53.577111Z",
     "shell.execute_reply.started": "2026-01-24T18:23:53.569686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading best_model.pth...\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_f8947c8c-3d10-4e81-8773-9992fbbf9205\", \"best_model.pth\", 69461045)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Download complete!\n"
     ]
    }
   ],
   "source": [
    "# Download the trained model\n",
    "from google.colab import files\n",
    "\n",
    "if os.path.exists(\"models/best_model.pth\"):\n",
    "    print(\"üì• Downloading best_model.pth...\")\n",
    "    files.download(\"models/best_model.pth\")\n",
    "    print(\"‚úÖ Download complete!\")\n",
    "else:\n",
    "    print(\"‚ùå Model not found. Please train the model first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3120670,
     "sourceId": 5380830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
