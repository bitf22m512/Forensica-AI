# ğŸ§¬ **ForensicaAI â€“ Deepfake Video Detection Using CNN + LSTM (Spatialâ€“Temporal Model)**

ForensicaAI is a lightweight, CPU-friendly deepfake detection prototype built using a **hybrid spatialâ€“temporal architecture**:
a **Convolutional Neural Network (CNN)** for frame-level spatial feature extraction and an **LSTM (RNN)** for temporal sequence modeling across video frames.

Designed for academic research and prototype development, this system processes raw videos, extracts frames, learns visual artifacts commonly found in deepfake content, and produces a **Real/Fake** prediction with confidence.

---

## ğŸ“Œ **Key Features**

* âœ” **End-to-end deepfake detection pipeline**
* âœ” Lightweight CNN for spatial feature extraction
* âœ” LSTM sequence model for capturing temporal inconsistencies
* âœ” Supports any dataset with `video,label` format
* âœ” CPU-friendly (works on low-end laptops)
* âœ” Jupyter Notebook included for training, testing, and inference
* âœ” Modular & easy to extend to ViT, MobileNet, EfficientNet, etc.
* âœ” Clean folder structure for academic submission

---

# ğŸ” **1. Project Description**

Deepfakes use generative models to manipulate human faces in videos. While visually convincing, these fakes often contain subtle artifacts in:

* **Spatial domain (frame-level)**

  * Texture irregularities
  * Lighting inconsistencies
  * Blending boundaries
  * GAN-specific fingerprints

* **Temporal domain (across frames)**

  * Unnatural blinking
  * Irregular lip motion
  * Inconsistent identity features
  * Sudden frame transitions

To detect such anomalies, ForensicaAI uses a **hybrid CNNâ€“LSTM model**:

* CNN processes each frame and extracts a 256-dimensional spatial embedding.
* LSTM reads the sequence of embeddings and detects temporal abnormalities.
* Final classifier outputs **REAL** or **FAKE** with confidence.

This makes the system ideal for forensic analysis, research, and early-stage product prototyping.

---

# ğŸ¯ **2. Problem Statement**

With the rapid evolution of deepfake generation techniques, it has become increasingly difficult to identify manipulated videos using traditional visual inspection. The lack of automated, accessible deepfake detection systems poses significant risks to:

* Public trust
* Media authenticity
* Cybersecurity
* Legal proceedings
* Personal identity/privacy

**Goal:**
Build a practical and efficient deepfake video detection model that works on small-scale hardware, while providing strong spatialâ€“temporal analysis.

---

# ğŸ“ **3. Dataset Description**

The system expects a dataset structured as:

```
data/
 â”œâ”€â”€ raw_videos/
 â”‚     â”œâ”€â”€ video1.mp4
 â”‚     â”œâ”€â”€ video2.mp4
 â”‚     â””â”€â”€ ...
 â”œâ”€â”€ frames/
 â””â”€â”€ labels.csv
```

### Celeb-DF-v2 quick start

1. Place the original dataset at `Celeb-DF-v2/` (sibling to this repo) with `Celeb-real` and `Celeb-synthesis` inside.
2. Build labels and copy videos into the project layout:
   ```
   python src/data_prep/prepare_celeb_df_v2.py --celeb_root Celeb-DF-v2
   ```
   This writes `data/labels.csv` (`video,label`) and fills `data/raw_videos/` with mp4s.
3. Extract frames into `data/frames/<video_name>/frame_i.jpg`:
   ```
   python src/data_prep/extract_frames.py
   ```
4. Train/evaluate using the generated frames and labels.

Configurable paths live in `config.yaml` (`raw_video_dir`, `frames_dir`, `labels_csv`, `num_frames`, `frame_size`).

### **labels.csv format**

```
video,label
video1.mp4,0
video2.mp4,1
...
```

Where:

* **0 = REAL**
* **1 = FAKE**

### Supported datasets:

* FaceForensics++ (manually preprocessed)
* DFDC Preview dataset
* Celeb-DF
* Custom recorded dataset

### Final dataset example used:

* **500 videos**
* 20 frames extracted per video
* 10,000 processed image frames in total

---

# ğŸ§  **4. System Architecture**

```
Raw Video
   â”‚
   â”œâ”€â”€ Frame Extraction (20 frames per video)
   â”‚
   â”œâ”€â”€ CNN (Spatial Feature Extractor)
   â”‚       â†“ 256-dim embedding
   â”‚
   â”œâ”€â”€ LSTM (Temporal Sequence Model)
   â”‚       â†“
   â”œâ”€â”€ Fully Connected Layer
   â”‚
   â””â”€â”€ Output: Real / Fake + Confidence
```

### **Spatial Model (CNN)**

* Lightweight 3-layer convolutional encoder
* Downsamples frames by factor of 8
* Outputs fixed 256-dimensional vectors

### **Temporal Model (LSTM)**

* Hidden size: 128
* Reads sequences of 20 frames
* Learns motion/consistency patterns

---

# âš™ï¸ **5. Installation**

Clone the repository:

```bash
git clone https://github.com/yourusername/ForensicaAI.git
cd ForensicaAI
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Main libraries used:

* Python 3.8+
* PyTorch
* NumPy
* Pandas
* OpenCV
* Matplotlib
* Pillow

---

# ğŸ› ï¸ **6. Usage Guide**

## **A. Extract Frames**

```python
python extract_frames.py
```

Or run the extraction cell in the Jupyter Notebook.

## **B. Train Model**

Run the notebook:

```
ForensicaAI_OptionA_Prototype.ipynb
```

Or train using CLI:

```python
python train.py
```

## **C. Evaluate Model**

```python
python evaluate.py
```

## **D. Run Streamlit App (Video Upload & Detection)**

The easiest way to use the trained model is through the Streamlit web interface:

**Windows:**
```bash
run_app.bat
```

**Linux/Mac:**
```bash
chmod +x run_app.sh
./run_app.sh
```

**Or manually:**
```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`. You can:
- Upload video files (MP4, AVI, MOV, MKV)
- Get real-time predictions (REAL/FAKE)
- View confidence scores and detailed reports
- Download analysis reports

**Note:** Make sure `models/best_model.pth` exists before running the app.

## **E. Command Line Inference (Alternative)**

```python
python src/inference.py
```

Then enter the video path when prompted.

Output example:

```
Prediction: FAKE
Confidence: 0.94
```

---

# ğŸ“¦ **7. Folder Structure**

```
ForensicaAI/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_videos/
â”‚   â”œâ”€â”€ frames/
â”‚   â””â”€â”€ labels.csv
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pth
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ walkthrough.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â””â”€â”€ video_dataset.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ cnn_feature_extractor.py
â”‚   â”‚   â””â”€â”€ rnn_classifier.py
â”‚   â”œâ”€â”€ data_prep/
â”‚   â”‚   â”œâ”€â”€ extract_frames.py
â”‚   â”‚   â””â”€â”€ prepare_celeb_df_v2.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pth
â”‚
â”œâ”€â”€ app.py (Streamlit frontend)
â”œâ”€â”€ run_app.bat (Windows launcher)
â”œâ”€â”€ run_app.sh (Linux/Mac launcher)
â”œâ”€â”€ config.yaml
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

# ğŸ“Š **8. Model Training Summary**

| Property                   | Value                                |
| -------------------------- | ------------------------------------ |
| Number of Videos           | 500                                  |
| Frames per Video           | 20                                   |
| Total Frames               | 10,000                               |
| Batch Size                 | 4                                    |
| Resolution                 | 128Ã—128                              |
| Training Time (Laptop CPU) | ~1.5 â€“ 4 hours                       |
| Best Accuracy              | Depends on dataset; typically 70â€“85% |

---

# ğŸ”¬ **9. Results & Observations**

* CNN alone struggles with temporal deepfake artifacts
* LSTM significantly boosts detection accuracy
* Model is small enough to run entirely on CPU
* Adding precomputed CNN features drastically speeds up training
* Increasing frame count â†’ better temporal learning
* Next step: apply hybrid CNN + ViT + LSTM or X3D architecture

---

# ğŸš€ **10. Future Work**

Several enhancements are planned:

* Add Vision Transformer (ViT) for global attention
* Add MobileNet/EfficientNet backbone for improved spatial encoding
* Improve temporal modeling using GRU or Temporal Convolution Networks
* Deploy model as lightweight REST API (FastAPI)
* Expand dataset to 2000+ videos for higher generalization

---

# ğŸ¤ **11. Credits**

This project was developed as part of a Forensic AI research prototype / Final Year Project.

Special thanks to:

* PyTorch Open Source Community
* FaceForensics++ & DFDC datasets
* Vision researchers contributing to spatialâ€“temporal deepfake detection

---

# â¤ï¸ **12. License**

You may use this code for academic, research, and non-commercial purposes.
Proper citation is appreciated when using this work.
